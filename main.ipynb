{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coralie/canada/Latent-truth-detector/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ISTA-DASLab/Llama-2-7b-AQLM-2Bit-2x8-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ISTA-DASLab/Llama-2-7b-AQLM-2Bit-2x8-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'CPU'\n",
    "print('Device: {} '.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): QuantizedLinear()\n",
       "          (k_proj): QuantizedLinear()\n",
       "          (v_proj): QuantizedLinear()\n",
       "          (o_proj): QuantizedLinear()\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantizedLinear()\n",
       "          (up_proj): QuantizedLinear()\n",
       "          (down_proj): QuantizedLinear()\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {}\n",
    "def capture_output(module, input, output):\n",
    "    activations['layer_13_output'] = output[0].detach()\n",
    "    \n",
    "hook = model.model.layers[13].register_forward_hook(capture_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"paris is in france\"\n",
    "input_token = tokenizer.encode_plus(input_text, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = input_token[\"input_ids\"].to(device)\n",
    "mask = input_token[\"attention_mask\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input_ids=input_ids, attention_mask=mask)\n",
    "    \n",
    "layer_13_output = activations['layer_13_output']\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 7, 4096]),\n",
       " tensor([[   1,  610,  275,  338,  297, 2524,  346]], device='cuda:0'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_13_output.shape , input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dimension 0 : taille du batch\n",
    "- dimension 1 : taille de la séquence\n",
    "- dimension 2 : taille de la représentation\n",
    "\n",
    "On aura donc des tailles variables de sortie en fonction de la taille de la séquence d'entrée. \n",
    "\n",
    "Comment faire pour l'utiliser pour la classification ?\n",
    "\n",
    "- utiliser le token `[CLS]` (modéle basé sur bert) qui est le premier token de la séquence (espece de représentation de la séquence entière) nous on est sur sententpiece `<s>` pas le meme role mais peut etre qu'on peut l'utiliser de la meme facon\n",
    "- average pooling : moyenne des représentations des tokens de la séquence\n",
    "- max pooling : max des représentations des tokens de la séquence\n",
    "- system d'attention : on va donner un poids à chaque token de la séquence et on va faire une somme pondérée des représentations des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens encodés: {'input_ids': tensor([[    1,  1222,   331,   552,   316, 19696,   371]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> Exemple de texte'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"Exemple de texte\", return_tensors=\"pt\")\n",
    "print(\"Tokens encodés:\", encoded_input)\n",
    "\n",
    "tokenizer.decode(encoded_input[\"input_ids\"].tolist()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
